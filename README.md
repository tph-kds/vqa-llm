<h1 align="center">VISUAL QUESTION ANSWERING - LLM</h1>

![](https://github.com/tph-kds/vqa-llm/blob/main/others/images/title_name.png)

# Bert + Vit-Google : Answer some issues from Question + realted Image by MultiModel Training
An implementation of the vqa model almost described similarly to the following in the paper:
"VQA with Cascade of Self- and Co-Attention Blocks."

Mishra, Aakansha ; Anand, Ashish ; Guha, Prithwijit

Full text available at: https://arxiv.org/pdf/2302.14777

## Contents
* [Model Overview](#model-overview)
    * [Introduction](#introduction)
    * [Architecture](#architecture)
* [Getting Started](#getting-started)
    * [Install Required Packages](#install-required-packages)
    * [Prepare the Training Data](#prepare-the-training-data)
    * [Download Trained Model](#download-trained-model)
* [Inference and Demo](#Infer-and-Demo)

## Model Overview

### Introduction

Building a multi-model usually is a difficult challenge that scientists want to explore and conquer. With the quick development of technology and algorithms, the majority of current approaches can be easier than the previous time. This project was created with a big desire to bring many valuable results when merging many large language models (LLM) together and deeply related knowledge.    

VQA model is a deep neural network that learns and responds to users when they provide questions and related images, the model will answer reliance on those information. Namely, as below images:

<p align="center">
  <img align="center" src="https://github.com/tph-kds/image_storages/blob/d88caf2b45c0ef2eeee160608205344706c5f938/images/svgs/vqa-llm/examples.png" width="800">
  
</p>

### Architecture

<p align="center">
  <img align="center" src="https://github.com/tph-kds/image_storages/blob/d88caf2b45c0ef2eeee160608205344706c5f938/images/svgs/vqa-llm/architecture.png" width="800">
  
</p>

## Getting Started

### Install Required Packages
(It is recommended to install the dependencies under Conda environment.)  
* python 3.7, 3.8, 3.9  
* tqdm  
* pytorch==0.4.0
* torchvision
* cython
* matplotlib
* numpy
* scipy
* pyyaml
* packaging
* pycocotools
* tensorboardx
* h5py
* opencv-python
* streamlit
* pillow # for PIL

### Prepare the Training Data

Name | #Image | #Question | #Answer
| :------:| :------: | :------: | :-----: |
Train | 5000 | 248350 | 248350
Validation | 2500 | 121513 | 121513
Test | 2500 | 107395 | 107395

All of orignal file are formatted in JSON files, after handling data processing, they has formatted in CSV files.

The csv files have some attribution:

```json
[
    {
        "image_id": int,
        "question": str,
        "question_id": int,
        "answer" : str
    }
]
```


### Download Based Models

Google - Bert ( textual_feature_extractor ): BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. [Available on this link](https://huggingface.co/google-bert/bert-base-uncased)


Google - ViT ( visual_feature_extractor ): The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. [Available on this link](https://huggingface.co/google/vit-base-patch16-224-in21k)


## Generating Poems

## Results

## Acknowledgements

- [BERT Model](https://huggingface.co/docs/transformers/model_doc/bert)
- [Vision Transfomer - ViT](https://huggingface.co/docs/transformers/model_doc/vit)
- [MultiModel](https://research.google/blog/multimodel-multi-task-machine-learning-across-domains/)

- Logo is generated by [@tranphihung](https://github.com/tph-kds)

## Future Plans
Fine-tune on a larger dataset
Evaluation on downstream tasks
Experiment with different model sizes
Experiment with different serving frameworks: vLLM, TGI, etc.
Experiment with expanding the tokenizer and prepare for pre-training
Stay tuned for future releases as we are continuously working on improving the model, expanding the dataset, and adding new features.

Thank you for your interest in our project. We hope you find it useful. If you have any questions, please feel free to reach out to us at ngoanpham1196@gmail.com

## References

- PAPER OK-VQA: A Visual Question Answering Benchmark Requiring
External Knowledge
[Available on this link](https://arxiv.org/pdf/1906.00067). [Online; accessed June 1, 2024].

- PAPER OK-VQA: A Visual Question Answering Benchmark Requiring
External Knowledge
[Available on this link](https://arxiv.org/pdf/1906.00067). [Online; accessed June 1, 2024].



## Citation

If you find this repo useful in your research, please consider citing the following papers:

```latex
@article{mishra2023vqa,
  title={VQA with Cascade of Self-and Co-Attention Blocks},
  author={Mishra, Aakansha and Anand, Ashish and Guha, Prithwijit},
  journal={arXiv preprint arXiv:2302.14777},
  year={2023}
}
```
