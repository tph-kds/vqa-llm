<h1 align="center">VISUAL QUESTION ANSWERING - LLM</h1>
![Alt text](https://github.com/tph-kds/vqa-llm/blob/main/others/images/title_name.png)

# Bert + Vit-Google : Answer some issues from Question + realted Image by MultiModel Training
An implementation of the vqa model almost described similarly to the following in the paper:
"VQA with Cascade of Self- and Co-Attention Blocks."

Mishra, Aakansha ; Anand, Ashish ; Guha, Prithwijit

Full text available at: https://arxiv.org/pdf/2302.14777

## Contents
* [Model Overview](#model-overview)
    * [Introduction](#introduction)
    * [Architecture](#architecture)
* [Getting Started](#getting-started)
    * [Install Required Packages](#install-required-packages)
    * [Prepare the Training Data](#prepare-the-training-data)
    * [Download Trained Model](#download-trained-model)
* [Inference and Demo](#Infer-and-Demo)

## Model Overview

### Introduction


### Architecture

## Getting Started

### Install Required Packages
(It is recommended to install the dependencies under Conda environment.)  
* python 3.7, 3.8, 3.9  
* tqdm  
* pytorch==0.4.0
* torchvision
* cython
* matplotlib
* numpy
* scipy
* pyyaml
* packaging
* pycocotools
* tensorboardx
* h5py
* opencv-python
* streamlit
* pillow # for PIL

### Prepare the Training Data


### Download Trained Model

## Generating Poems

## Results


## Citation

If you find this repo useful in your research, please consider citing the following papers:

```latex
@article{mishra2023vqa,
  title={VQA with Cascade of Self-and Co-Attention Blocks},
  author={Mishra, Aakansha and Anand, Ashish and Guha, Prithwijit},
  journal={arXiv preprint arXiv:2302.14777},
  year={2023}
}
```
